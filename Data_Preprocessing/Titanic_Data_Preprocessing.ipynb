{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f829923-f33f-4967-beef-08d1f715a0e0",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "# Titanic Survival Data Preprocessing\n",
        "\n",
        "In this notebook, we will go through the process of data preprocessing on the Titanic survival dataset. Data preprocessing is a crucial step in any machine learning project. It involves cleaning the data and making it suitable for a machine learning model which can enhance the accuracy of the predictions.\n",
        "\n",
        "The steps we are going to perform are:\n",
        "\n",
        "1. **Loading the data:** We will load the data into a pandas DataFrame.\n",
        "2. **Missing value handling:** We will check for any missing values in the dataset and figure out how to handle them.\n",
        "3. **Data cleaning:** This step involves removing errors and inconsistencies from the data.\n",
        "4. **Feature engineering:** We will create new features from existing ones to provide more valuable data to the machine learning model.\n",
        "5. **Feature Scaling:** We will standardize or normalize the range of independent variables or features of data.\n",
        "6. **Categorical to Numerical feature conversion:** We will convert categorical data to numerical data, as machine learning models work better with numerical data.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c43eebe0-ed6f-4727-b063-e616779a4ff4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:32:37.934392+00:00",
          "start_time": "2023-08-04T13:32:37.700708+00:00"
        },
        "datalink": {
          "c66ce0cf-9a11-454d-818a-eb603297c6d2": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 12,
              "orig_num_rows": 5,
              "orig_size_bytes": 520,
              "truncated_num_cols": 12,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 520,
              "truncated_string_columns": []
            },
            "display_id": "c66ce0cf-9a11-454d-818a-eb603297c6d2",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-04T13:32:37.774383",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_569643f5717e43599c241aff3b5a7e02"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "15aac936-b0fd-417c-973c-4e6855e0c3a7"
        }
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Loading the data\n",
        "df = pd.read_csv('titanic.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e14a17-e247-49d4-b319-685efba09c79",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "We have loaded the Titanic survival dataset into a pandas DataFrame. A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. It is similar to a spreadsheet or SQL table, or a dictionary of Series objects. It is generally the most commonly used pandas object.\n",
        "\n",
        "Here's a brief description of the columns in the DataFrame:\n",
        "\n",
        "- **PassengerId:** An unique index for passenger rows. It starts from 1 and increments by 1 for every new passenger.\n",
        "- **Survived:** Shows if the passenger survived or not. 1 stands for survived and 0 stands for not survived.\n",
        "- **Pclass:** Ticket class. 1 stands for First class ticket. 2 stands for Second class ticket. 3 stands for Third class ticket.\n",
        "- **Name:** Passenger's name. Name also contain title. 'Mr' for man. 'Mrs' for woman. 'Miss' for girl. 'Master' for boy.\n",
        "- **Sex:** Passenger's sex. It's either Male or Female.\n",
        "- **Age:** Passenger's age. 'NaN' values in this column indicates that the age of that particular passenger has not been recorded.\n",
        "- **SibSp:** Number of siblings or spouses travelling with each passenger.\n",
        "- **Parch:** Number of parents or children travelling with each passenger.\n",
        "- **Ticket:** Ticket number.\n",
        "\n",
        "Let's move to the next step - handling missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2814279f-a41b-438d-a2b1-370f8921c30b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:33:19.527929+00:00",
          "start_time": "2023-08-04T13:33:19.340629+00:00"
        },
        "datalink": {
          "2424a0fc-7a3c-41e5-889e-b0a20cea7af4": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 1,
              "orig_num_rows": 12,
              "orig_size_bytes": 192,
              "truncated_num_cols": 1,
              "truncated_num_rows": 12,
              "truncated_size_bytes": 192,
              "truncated_string_columns": []
            },
            "display_id": "2424a0fc-7a3c-41e5-889e-b0a20cea7af4",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-04T13:33:19.372203",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_275b47757d4c41f49e5dbe84bc26b110"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "53869c9f-af19-4f35-b399-eb48994f22c1"
        }
      },
      "outputs": [],
      "source": [
        "# Checking for missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9463f619",
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can also visulazize the missing values using seaborn library\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (10,8))\n",
        "sns.heatmap(df.isnull(), cbar = False, cmap = 'viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c02fcf-aa76-44cd-adc2-3ffbfb84126a",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Missing Value Handling\n",
        "\n",
        "Missing values in the dataset are a common issue in data preprocessing. They refer to the values that are not present or are null. Handling missing values is important as they can lead to wrong prediction or classification for any given model being used.\n",
        "\n",
        "The `isnull().sum()` function in pandas is used to check and manage NULL values in a data frame. It returns the number of missing values in the data set.\n",
        "\n",
        "From the output, we can see that there are 177 missing values in the 'Age' column, and 687 missing values in the 'Cabin' column. We need to decide how to handle these missing values.\n",
        "\n",
        "One common way is to fill the missing values with the mean (average) of the non-missing values in the column. This is known as **mean imputation**. However, this method may not be suitable for our 'Age' column as the age can't be fractional.\n",
        "\n",
        "Another way is to fill the missing values with the median (middle value) of the non-missing values. This is known as **median imputation**. This method is more robust to outliers and skewed data.\n",
        "\n",
        "For the 'Cabin' column, since there are a lot of missing values, it might be better to drop this column as it may not provide much useful information for the model.\n",
        "\n",
        "Let's perform median imputation for the 'Age' column and drop the 'Cabin' column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e5a9e28-131d-4017-9a84-504fb3bf38cf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:34:10.722170+00:00",
          "start_time": "2023-08-04T13:34:10.531121+00:00"
        },
        "datalink": {
          "4a127af0-caa9-4a75-97f2-ca5ce9b1e5a9": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 1,
              "orig_num_rows": 11,
              "orig_size_bytes": 176,
              "truncated_num_cols": 1,
              "truncated_num_rows": 11,
              "truncated_size_bytes": 176,
              "truncated_string_columns": []
            },
            "display_id": "4a127af0-caa9-4a75-97f2-ca5ce9b1e5a9",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-04T13:34:10.565816",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_1ee5d10b36b545939c05934d29620a45"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "127808f7-7a5b-452c-a7cb-470e0347cc40"
        }
      },
      "outputs": [],
      "source": [
        "# Filling missing Age values with median\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "\n",
        "# Dropping the Cabin column\n",
        "df = df.drop('Cabin', axis=1)\n",
        "\n",
        "# Checking for missing values again\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "457a6c82-c22e-48d2-a5b3-dba5f6a818ac",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Missing Value Handling (Continued)\n",
        "\n",
        "We have filled the missing values in the 'Age' column with the median age, and dropped the 'Cabin' column from the DataFrame. Now, as we can see from the output, there are no missing values in our dataset.\n",
        "\n",
        "Next, let's move to data cleaning. In this step, we will check for any errors or inconsistencies in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbe11b26-79c9-4b81-9dbd-2d36a0af7e3c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:34:34.489880+00:00",
          "start_time": "2023-08-04T13:34:34.325834+00:00"
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c7110f37-4807-4237-9053-5e00e443ea47"
        }
      },
      "outputs": [],
      "source": [
        "# Checking for any duplicates\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d8a0cb8-4a23-44ae-915e-3ca56102fefc",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset. This involves identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.\n",
        "\n",
        "One common issue is duplicate entries in the data. We checked for duplicates in our dataset using the `duplicated().sum()` function, which returns the number of duplicate rows. As we can see from the output, there are no duplicate rows in our dataset, so we can move on to the next step.\n",
        "\n",
        "Next, let's perform feature engineering. In this step, we will create new features from existing ones to provide more valuable data to the machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83fa86b1-2c84-49b1-aeea-d643101f5ce8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:35:17.883925+00:00",
          "start_time": "2023-08-04T13:35:17.645301+00:00"
        },
        "datalink": {
          "8c2c0557-e2c2-4071-b534-e83ad5f54b20": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 13,
              "orig_num_rows": 5,
              "orig_size_bytes": 560,
              "truncated_num_cols": 13,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 560,
              "truncated_string_columns": []
            },
            "display_id": "8c2c0557-e2c2-4071-b534-e83ad5f54b20",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-04T13:35:17.723357",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_e7067123bbc5414a8cabfa9d9bcb8a3b"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "bb629fac-d54d-4529-91b1-01ba2ba13911"
        }
      },
      "outputs": [],
      "source": [
        "# Creating a new feature 'FamilySize' from 'SibSp' and 'Parch'\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "# Creating a new feature 'IsAlone' to check if the passenger is alone on the ship\n",
        "df['IsAlone'] = 0\n",
        "df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "\n",
        "# Checking the updated DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90874094-eecf-4898-9065-27f5bca3381a",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Feature engineering is the process of using domain knowledge to extract features from raw data. These features can be used to improve the performance of machine learning algorithms. Features are characteristics or properties shared by all the independent units on which analysis or prediction is to be done.\n",
        "\n",
        "We have created two new features:\n",
        "\n",
        "1. **FamilySize:** This is a combination of 'SibSp' (number of siblings or spouses aboard) and 'Parch' (number of parents or children aboard). We add 1 to include the passenger themself. This feature represents the total size of the family aboard.\n",
        "\n",
        "2. **IsAlone:** This is a binary feature derived from 'FamilySize'. If the 'FamilySize' is 1, then the 'IsAlone' will be 1, indicating that the passenger is alone. Otherwise, 'IsAlone' will be 0.\n",
        "\n",
        "These new features can provide additional information to the machine learning model, potentially improving its performance.\n",
        "\n",
        "Next, let's move to feature scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603e9593-37cd-48c8-9f48-b46e47db60af",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:36:07.904258+00:00",
          "start_time": "2023-08-04T13:36:07.332689+00:00"
        },
        "datalink": {
          "b8ebe395-5a84-481b-8d31-3ffde8400cca": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 13,
              "orig_num_rows": 5,
              "orig_size_bytes": 560,
              "truncated_num_cols": 13,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 560,
              "truncated_string_columns": []
            },
            "display_id": "b8ebe395-5a84-481b-8d31-3ffde8400cca",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-04T13:36:07.745289",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_16750ec3408c4d0d96734b572b9ed653"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "aee74c89-5499-4fff-adf4-46b361dd2cc5"
        }
      },
      "outputs": [],
      "source": [
        "# Importing the necessary library for feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Creating a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Applying the scaler to the 'Age' and 'Fare' columns\n",
        "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\n",
        "\n",
        "# Checking the updated DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5589c6-7071-4bb5-b097-7350f0aa4583",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
        "\n",
        "We have applied feature scaling to the 'Age' and 'Fare' columns. These two features were chosen because they are numerical features with different scales. 'Age' ranges from about 0 to 80, while 'Fare' can range from 0 to much higher values. By scaling these features, we ensure that they contribute equally to the model's performance.\n",
        "\n",
        "We used the StandardScaler from the sklearn.preprocessing module, which standardizes features by removing the mean and scaling to unit variance. The standard score of a sample x is calculated as:\n",
        "\n",
        "z = (x - u) / s\n",
        "\n",
        "where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n",
        "\n",
        "Next, let's convert categorical features to numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21448b35-1542-42d9-adad-e61203535a7d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:36:49.768330+00:00",
          "start_time": "2023-08-04T13:36:49.542421+00:00"
        },
        "datalink": {
          "3c89ed74-9074-4f01-9461-9a2f0d7915b2": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 13,
              "orig_num_rows": 5,
              "orig_size_bytes": 560,
              "truncated_num_cols": 13,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 560,
              "truncated_string_columns": []
            },
            "display_id": "3c89ed74-9074-4f01-9461-9a2f0d7915b2",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-04T13:36:49.609646",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_27122126df984cb6af6f9d07dd1e55ae"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "452bc745-d70d-49f9-98b2-57d0294f9db3"
        }
      },
      "outputs": [],
      "source": [
        "# Importing the necessary library for label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Creating a LabelEncoder instance\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Applying the encoder to the 'Sex' column\n",
        "df['Sex'] = encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Checking the updated DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca31ca5-7c0a-405b-aadb-cb27d13a22b0",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Categorical to Numerical Feature Conversion\n",
        "\n",
        "Machine learning models require input and output variables to be numeric. This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.\n",
        "\n",
        "We have converted the 'Sex' column from categorical to numerical. The 'Sex' column contains two categories: 'male' and 'female'. We used the LabelEncoder from the sklearn.preprocessing module, which encodes target labels with value between 0 and n_classes-1. In our case, 'male' is encoded as 1 and 'female' as 0.\n",
        "\n",
        "Now, all the features in our dataset are numerical and ready to be used for a machine learning model.\n",
        "\n",
        "This concludes our data preprocessing. We have loaded the data, handled missing values, cleaned the data, engineered features, scaled features, and converted categorical features to numerical features. The dataset is now ready for use in a machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30e88be-6595-4906-b306-5280ea46a84e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-04T13:38:01.949790+00:00",
          "start_time": "2023-08-04T13:38:01.719708+00:00"
        },
        "datalink": {
          "9a635bd6-73fd-4ed3-a3c3-6979b6dd2efe": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 13,
              "orig_num_rows": 5,
              "orig_size_bytes": 560,
              "truncated_num_cols": 13,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 560,
              "truncated_string_columns": []
            },
            "display_id": "9a635bd6-73fd-4ed3-a3c3-6979b6dd2efe",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-04T13:38:01.788831",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_3d168a160d3f477aa9faa807eda7882b"
          }
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "fa2accb1-a0f1-4b57-bd11-a8a6bbac16fc"
        }
      },
      "outputs": [],
      "source": [
        "# Importing the necessary library for label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Creating a LabelEncoder instance\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Applying the encoder to the 'Sex' column\n",
        "df['Sex'] = encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Checking the updated DataFrame\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "3c7b0f15-ac32-4a26-9e61-cee6bb901a18"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "5cbac3fe-2d59-55c3-b36d-6667e28fafe7",
        "openai_ephemeral_user_id": "06261dfa-ecfa-59c8-b689-1ecbf6009bc2",
        "openai_subdivision1_iso_code": "TH-10"
      }
    },
    "selected_hardware_size": "small"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
